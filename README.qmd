---
format: gfm
bibliography: bibliography.bib
execute: 
  message: false
  warning: false
# knitr:
#   opts_chunk:
#     fig.path: "man/figures/README-"
# eval: false
default-image-extension: ""
---

# flowcluster

<!-- badges: start -->

[![R-CMD-check](https://github.com/Hussein-Mahfouz/flowcluster/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/Hussein-Mahfouz/flowcluster/actions/workflows/R-CMD-check.yaml)

<!-- badges: end -->

The goal of flowcluster is to provide minimal functionality for clustering OD desire lines (or flows). This includes: 1. Creating distance matrices between OD pairs 2. Passing distance matrices to a clustering algorithm

## Install from GitHub

You can install the development version of flowcluster from GitHub with:

```{r}
#| eval=FALSE
# install.packages("devtools")
devtools::install_github("Hussein-Mahfouz/flowcluster")
```

Load it as follows:

```{r}
#| eval=FALSE
library(flowcluster)
```

## Usage

The package provides a function to create a distance matrix from a data frame of OD pairs, and then pass that matrix to a clustering algorithm. The current distance matrix is an implementation of the flow distance and flow dissimilarity measures described in [@tao2016spatial].


### Data preparation

First, load your package and the sample data, and project it to a metric CRS.  
This is important for accurate length calculations and spatial operations.

```{r}
library(flowcluster)
library(sf)
library(tmap)

# Load sample flow data and project to metric CRS (e.g., EPSG:3857)
flows_sf <- flows_leeds
flows_sf <- st_transform(flows_sf, 3857)
# # let's work with a subset of the data
flows_sf <- flows_sf[1:1000, ]
```

Next, add a column containing the length (in meters) of each flow line.  
This step also checks that your data is in a projected CRS.

```{r}
# Add length (meters) to each flow line
flows_sf <- add_flow_length(flows_sf)
head(flows, 5)
```

Filter out flows based on a minimum and maximum length.

```{r}
# Filter flows based on length (e.g., between 100 and 10000 meters)
flows_sf = filter_by_length(flows_sf, length_min = 1000, length_max = 20000)
```
Then, extract start and end coordinates for each flow, and assign unique IDs.  
These are needed for subsequent distance calculations and clustering.

```{r}
# Add start/end coordinates and unique flow IDs
flows_sf <- add_xyuv(flows_sf)
head(flows_sf, 5)
```

### Distance Matrix calculation

Calculate a pairwise distance measure between all flows using their coordinates.  
You can adjust `alpha` and `beta` to change the weighting of start and end locations.

```{r}
# Compute pairwise flow distances (fd and fds columns)
flows = st_drop_geometry(flows_sf)
distances <- flow_distance(flows, alpha = 1, beta = 1)
```

Convert the long-format distance table to a matrix, which is required for clustering.

```{r}
# Create a distance matrix from the long-form distance data. Choose the column for distances you want to use.
# The 'fds' column is the flow dissimilarity measure, and the 'fd' column is the flow distance measure.
dmat <- distance_matrix(distances, distance_col = "fds")
dmat
```

### Clustering 

Prepare a weight vector, typically based on a "count" column (number of trips, etc).  
If your data does not have a "count" column, you can add one with `flows$count <- 1`. 
Weights are very handy, otherwise our matrix would be huge, as we would have to replicate 
each OD pair n times depending on the number of observations between them. Unfortunately, 
there is no out of the 

```{r}
# Prepare weights for each flow (here we use the count column)
wvec <- weight_vector(dmat, flows, weight_col = "count")
```

Finally, cluster the flows using DBSCAN.  
Adjust `eps` and `minPts` to control cluster tightness and minimum cluster size. Cluster composition is 
greatly affected by these prameters. 

```{r}
# Cluster flows using DBSCAN
flows_clustered <- cluster_flows_dbscan(dmat, wvec, flows, eps = 8, minPts = 70)

# View the first few rows of the clustered data
head(flows_clustered, 10)

# how many unique clusters were found?
length(unique(flows_clustered$cluster))

# number of flows in each cluster
flows_clustered |> 
  group_by(cluster) |> 
  summarise(n = n()) |> 
  arrange(desc(n))
```

### Visualise 

Let's take a look at the clusters


```{r}
# Keep only the biggest clusters for visualisation
flows_clustered = flows_clustered |>
  filter(cluster != 0) |> # these are normally the noisepoints
  group_by(cluster) |>
  mutate(size = n(), 
         count_cluster = sum(count)) |>
  ungroup() |>
  filter(size > 7, # minimum size of cluster
         count_cluster > 100) # minumum number of trips in cluster
  

```

Add geometry back onto the data 

```{r}
# Add the geometry back onto the data
flows_clustered = flows_sf |>
  select(flow_ID) |>
  inner_join(flows_clustered, by = "flow_ID") 
```



```{r}

# plot
 tm_shape(flows_clustered) +
  tm_lines(lwd = "count",
           col = "cluster",
           palette = "Accent", #YlGn
           #style = "pretty",
           alpha = 1,
           title.col = "Cluster",
           title.lwd = "No. of people",
           legend.col.show = FALSE,
           showNA = FALSE) +
  tm_facets(by = "cluster",
            free.coords = FALSE,
            nrow = 3,
            showNA = FALSE) +
  tm_layout(fontfamily = 'Georgia',
            main.title = paste0("Clustered flows"),
            main.title.size = 1.1,
            main.title.color = "azure4",
            main.title.position = "left",
            legend.outside = TRUE,
            legend.outside.position = "bottom",
            legend.stack = "horizontal",
            # remove panel headers
            #panel.show = FALSE,
            frame = FALSE) 

```

## Future Work

- [ ] Sensitivity function to show how clustering changes with different `eps` and `minPts` values
- [ ] Add more distance matrices (e.g. Frechet distance)
- [ ] Add more clustering algorithms, and use a more efficient data structure as a workaround for not being able 
to use weights with other clustering algorithms

